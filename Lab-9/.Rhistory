}
}
primeOrNot(inp)
primeOrNot<-function(inp){
flag = TRUE
for(z in 2:(inp-1) ){
if((inp %% z) == 0){
print(z)
flag = FALSE
}
print(z)
}
if(flag){
print(paste(inp,"is prime"))
}else{
print(paste(inp,"is not prime"))
}
}
primeOrNot(inp)
A <- matrix(1:6, nrow=2)
A
B<- matrix(1:6,ncol=2)
B
A%*%B
PL <- iris$Petal.Length
barplot(PL)
hist(PL)
SP <- iris$Species
boxplot(PL ~ SP)
summary(aov(PL ~ SP))
PW <- df$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
PW <- iris$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
heatmap(iris)
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
ma <- as.matrix(iris[, 1:4]) # convert to matrix
disMatarix <- dist(ma)
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
age <- c(23,23,27,27,39,41,49,50,52,54,54,56,57,58,58,60,61)
age <- c(23,23,27,27,39,41,49,50,52,54,54,56,57,58,58,60,61)
fatpercent <- c(9.5,26.5,7.8,17.8,31.4,25.9,27.4,27.2,31.2,34.6,42.5,28.8,33.4,30.2,34.1,32.9,41.2,35.7)
data <- list(A = age,B = fatpercent)
df <- as.data.frame(data)
data
age <- c(23,23,27,27,39,41,47,49,50,52,54,54,56,57,58,58,60,61)
fatpercent <- c(9.5,26.5,7.8,17.8,31.4,25.9,27.4,27.2,31.2,34.6,42.5,28.8,33.4,30.2,34.1,32.9,41.2,35.7)
data <- list(A = age,B = fatpercent)
data
df <- as.data.frame(data)
df
data <- list(Age = age,%fat = fatpercent)
data <- list(Age = age,FatPercent = fatpercent)
data
df <- as.data.frame(data)
df
cor(df)
cor(df$Age,df$FatPercent)
# Histogram
hist(iris$Petal.Length,breaks = 30,col="blue", xlab = "Petal Length", main = " Histogram of Pertal Length" )
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
# importing plotting lib
require(ggplot2)
# Density plot
qplot(Petal.Length, data=iris, geom='density')
qplot(Petal.Length, data=iris, geom='density', color=Species, fill=Species)
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
# Density plot
qplot(Petal.Length, data=iris, geom='density')
qplot(Petal.Length, data=iris, geom='density', color=Species, fill=Species)
qplot(Petal.Width, Petal.Length, data=iris, color=Species)
# Scatter plot
qplot(Petal.Width, Petal.Length, data=iris, color=Species)
cor(iris$Sepal.Length, iris$Petal.Length)
cov(iris$Sepal.Length, iris$Petal.Length)
# correlation and covariance
cor(iris$Sepal.Length, iris$Petal.Length)
cov(iris$Sepal.Length, iris$Petal.Length)
cov(iris[,1:4])
df <- iris[, 1:4]
boxplot(df)
pairs(df)
stars(df)
# making every plot i know of ;)
df <- iris[, 1:4]
boxplot(df)
pairs(df)
stars(df)
boxplot(iris)
scatter.smooth(iris)
matrix(scatter.smooth(iris))
table(iris)
pie(table(iris))
matrix(scatter.smooth(iris))
matrix(scatter.smooth(iris))
scatter.smooth(iris)
matrix(scatter.smooth(iris))
table(iris)
table(iris)
pie(table(iris))
barplot(PL)
hist(PL)
SP <- iris$Species
boxplot(PL ~ SP)
summary(aov(PL ~ SP))
PW <- iris$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
ma <- as.matrix(iris[, 1:4]) # convert to matrix
disMatarix <- dist(ma)
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
vec1<-c(1.1, 2, 3.0, 4.2)
sum(vec1)
prod(vec1)
mean(vec1)
vec2<-c(1.1,NA, 2, 3.0,NA )
sum(vec2)
prod(vec2)
mean(vec2)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
count(str,"%")
count(str,%)
str_count(str)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str)
str_count("%")
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count("%")
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str,pattern = "%")
install.packages("stringr")
library(stringr)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str,pattern = "%")
str_count(str,pattern = 'r')
ar <- array(1:5,c(2,3,4))
ar
dim(ar)
for(i in 1:100){
if(i%%4==0 && i%%5==0){
print("BuyNow ")
}
else if(i%%4==0){
print("Now ")
}
else if(i%%5==0){
print("Buy ")
}
else{
print(i)
print(" ")
}
}
for(i in 1:100){
if(i%%4==0 && i%%5==0){
print("BuyNow")
}
else if(i%%4==0){
print("Now")
}
else if(i%%5==0){
print("Buy")
}
else{
print(i)
}
}
print(Sys.Date())
print(Sys.time())
mtcars
data<-mtcars
View(data)
library(dplyr)
select(data)
data<-mtcars
View(data)
dim(data)
summary(data)
randu()
runif(2,1,32)
floor(runif(2,1,32))
floor(runif(2,1,32))
data %>% select(floor(runif(2,1,32))
data
data %>% select(floor(runif(2,1,32))
summary(data)
data %>% select(floor(runif(2,1,32)) )
data %>% select(floor(runif(2,1,1)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(11,1,11)) )
data %>% select(floor(runif(11,1,11)) )
data %>% row.select(floor(runif(11,1,11)) )
data %>% row(select(floor(runif(11,1,11))) )
data %>% (select(row, floor(runif(11,1,11))) )
data %>% filter(floor(runif(11,1,11)))
data %>% filter(floor(runif(2,1,32)))
data %>% filter(floor(runif(1,1,32)))
data %>% filter(2)
data %>% filter(32)
filter(data,)
filter(data,1)
filter(data,false)
slice_sample(data)
slice_sample(data)
slice_sample(data,n)
slice_sample(data,5)
slice_sample(data,n=5)
sample_frac(data,n=5)
sample_frac(data,n=2)
sample_frac(data,n=2)
slice_sample(data,n=5)
sample_frac(data,n=2)
data %>% select(cyl, hp, wt )
filter(mtcars, am == 1)
filter(mtcars,hp>225)
mean(mtcars$mpg)
median(mtcars$mpg)
arrange(mtcars,desc(hp))
days <- c("Monday","Tuesday","Wednesday","Thrusday","Friday","Saturdat","Sunday")
nrbirds <- c(2,5,0,8,1,2,3)
df<-data.frame(days,nrbirds)
df
dayNumber<-c(1,2,3,4,5,6,7)
x<-c(1,2,3,4,5,6,7)
df$dayNumber <- x
df
df = subset(df,select(days))
df = subset(df,select = days)
df
df = subset(df,select = -c(days))
df
days <- c("Monday","Tuesday","Wednesday","Thrusday","Friday","Saturdat","Sunday")
nrbirds <- c(2,5,0,8,1,2,3)
df<-data.frame(days,nrbirds)
x<-c(1,2,3,4,5,6,7)
df$dayNumber <- x
df = subset(df,select = -c(days))
df
df <- select(df, dayNumber, nrbirds)
df
max_row <- which.max(df$nrbirds)
print(max_row)
df
max_row <- which.max(df$nrbirds)
print(max_row)
df[max_row]
df[max_row]
most_birds_daynumber <- df$dayNumber[max_row]
print(most_birds_daynumber)
df <- arrange(df, desc(nrbirds))
df
library(dplyr)
library(stringr)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
library(neuralnet)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
# library(tidyverse)
# library(neuralnet)
iris <- iris %>% mutate_if(is.character, as.factor)
set.seed(245)
data_rows <- floor(0.80 * nrow(iris))
train_indices <- sample(c(1:nrow(iris)), data_rows)
train_data <- iris[train_indices,]
test_data <- iris[-train_indices,]
model = neuralnet(
Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=train_data,
hidden=c(4,2),
linear.output = FALSE
)
plot(model,rep = "best")
pred <- predict(model, test_data)
labels <- c("setosa", "versicolor", "virginca")
prediction_label <- data.frame(max.col(pred)) %>%
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()
table(test_data$Species, prediction_label)
check = as.numeric(test_data$Species) == max.col(pred)
accuracy = (sum(check)/nrow(test_data))*100
print(accuracy)
install.packages("mlbench")
library(mlbench)
data(PimaIndiansDiabetes)
# Some of the exact variable names may vary from my subsequent code
all_test_accuracies_knn <- matrix(nrow=100,ncol=9)
for (split_number in c(1:100)){
train_ind <- sample.split(diabetes$Pregnancies,SplitRatio = 0.8)
test_ind <- !train_ind
neighbors <- c(2:10)
accuracies <- matrix(nrow=1, ncol=9)
for (n_neighbors in neighbors){
knn_fit <- knn(diabetes[train_ind,],diabetes[test_ind,],diabetes$Outcome[train_ind],k=n_neighbors)
cm <- table(Actual = diabetes$Outcome[test_ind],Predicted = knn_fit)
accuracy <- sum(diag(cm))/sum(test_ind)
accuracies[n_neighbors-1] <- accuracy
}
all_test_accuracies_knn[split_number,] <- accuracies
}
train_ind <- Sample.split(diabetes$Pregnancies,SplitRatio = 0.8)
detach("package:neuralnet", unload = TRUE)
for (split_number in c(1:100)){
train_ind <- sample.split(diabetes$Pregnancies,SplitRatio = 0.8)
test_ind <- !train_ind
neighbors <- c(2:10)
accuracies <- matrix(nrow=1, ncol=9)
for (n_neighbors in neighbors){
knn_fit <- knn(diabetes[train_ind,],diabetes[test_ind,],diabetes$Outcome[train_ind],k=n_neighbors)
cm <- table(Actual = diabetes$Outcome[test_ind],Predicted = knn_fit)
accuracy <- sum(diag(cm))/sum(test_ind)
accuracies[n_neighbors-1] <- accuracy
}
all_test_accuracies_knn[split_number,] <- accuracies
}
library(caTools)
# Some of the exact variable names may vary from my subsequent code
all_test_accuracies_knn <- matrix(nrow=100,ncol=9)
for (split_number in c(1:100)){
train_ind <- sample.split(diabetes$Pregnancies,SplitRatio = 0.8)
test_ind <- !train_ind
neighbors <- c(2:10)
accuracies <- matrix(nrow=1, ncol=9)
for (n_neighbors in neighbors){
knn_fit <- knn(diabetes[train_ind,],diabetes[test_ind,],diabetes$Outcome[train_ind],k=n_neighbors)
cm <- table(Actual = diabetes$Outcome[test_ind],Predicted = knn_fit)
accuracy <- sum(diag(cm))/sum(test_ind)
accuracies[n_neighbors-1] <- accuracy
}
all_test_accuracies_knn[split_number,] <- accuracies
}
diabetes = data(PimaIndiansDiabetes)
# Some of the exact variable names may vary from my subsequent code
all_test_accuracies_knn <- matrix(nrow=100,ncol=9)
for (split_number in c(1:100)){
train_ind <- sample.split(diabetes$Pregnancies,SplitRatio = 0.8)
test_ind <- !train_ind
neighbors <- c(2:10)
accuracies <- matrix(nrow=1, ncol=9)
for (n_neighbors in neighbors){
knn_fit <- knn(diabetes[train_ind,],diabetes[test_ind,],diabetes$Outcome[train_ind],k=n_neighbors)
cm <- table(Actual = diabetes$Outcome[test_ind],Predicted = knn_fit)
accuracy <- sum(diag(cm))/sum(test_ind)
accuracies[n_neighbors-1] <- accuracy
}
all_test_accuracies_knn[split_number,] <- accuracies
}
view(diabetes)
library(ggplot2)
library(caret)
install.packages("caret")
library(caret)
library(ggplot2)
library(caret)
library(caretEnsemble)
install.packages("caretEnsemble")
library(caretEnsemble)
library(psych)
install.packages("psych")
install.packages("Amelia")
install.packages("mice")
install.packages("GGally")
install.packages("rpart")
install.packages("randomForest")
install.packages("e1071")
install.packages("ROCR")
install.packages("partykit")
library(ggplot2)
library(caret)
library(caretEnsemble)
library(psych)
library(Amelia)
library(caretEnsemble)
library(psych)
library(Amelia)
library(mice)
library(GGally)
library(rpart)
library(randomForest)
library(e1071)
library(ROCR)
library(partykit)
RNGkind(sample.kind = "Rounding")
set.seed(123)
# setting the working directory to use the csv file
print(getwd())
setwd("C:/Users/ASUS/OneDrive/Desktop/Study-Material/IIIT-Kottayam/SEM-6/Data warehousing and mining ICS 321/Lab/Lab-9")
diabetes_df <- read.csv("diabetes.csv", header = TRUE, stringsAsFactors = T)
head(diabetes_df)
names(diabetes_df) <- c("pregnancies", "glucose", "bloodpressure", "skinthickness", "insulin", "bmi", "diabetespedigreefunction", "age", "outcome")
diabetes_df$outcome <- as.factor(diabetes_df$outcome) %>% na.omit()
#Change Outcome variables as categorical with level = True (1) or False (0)
diabetes_df$outcome <- factor(diabetes_df$outcome, levels = c(0,1), labels = c("False", "True"))
glimpse(diabetes_df)
head(diabetes_df)
#Dive deeper into the structure of the data
str(diabetes_df)
describe(diabetes_df)
colSums(is.na(diabetes_df))
sum(is.na(diabetes_df))
anyNA(diabetes_df)
#Replace '0' values into NA
diabetes_df[, 2:7][diabetes_df[, 2:7] == 0] <- NA
#Missing value in our dataset
diabetes_missmap <- missmap(diabetes_df)
#predict the missing values
diabetes_MICE <- mice(diabetes_df[, c("glucose","bloodpressure",
"skinthickness","insulin","bmi")],
method='rf')
diabetes_result <- complete(diabetes_MICE)
#add the predicted missing values into our data set
diabetes_df$glucose <- diabetes_result$glucose
diabetes_df$bloodpressure <- diabetes_result$bloodpressure
diabetes_df$skinthickness <- diabetes_result$skinthickness
diabetes_df$insulin  <- diabetes_result$insulin
diabetes_df$bmi <- diabetes_result$bmi
diabetes_missmap <- missmap(diabetes_df)
#Building a model
#split data into training and test data sets
sliced <- createDataPartition(y = diabetes_df$outcome,p = 0.75,list = FALSE)
diabetes_train <- diabetes_df[sliced,]
diabetes_test <- diabetes_df[-sliced,]
#Check dimensions of the split
prop.table(table(diabetes_df$outcome)) * 100
prop.table(table(diabetes_train$outcome)) * 100
prop.table(table(diabetes_test$outcome)) * 100
#create x_diabetes which holds the predictor variables and y which holds the response variables
x_diabetes = diabetes_train[,-9]
y_diabetes = diabetes_train$outcome
#Our Naive Bayes Model
diabetes_NB_model = train(x_diabetes,y_diabetes,'nb',trControl=trainControl(method='cv',number=10))
#Our Naive Bayes Model
diabetes_NB_model = train(x_diabetes,y_diabetes,'nb',trControl=trainControl(method='cv',number=10))
#Our Naive Bayes Model
diabetes_NB_model = train(x_diabetes,y_diabetes,'nb',trControl=trainControl(method='cv',number=10))
diabetes_NB_model
#Evaluation
confusionMatrix(diabetes_pred_NB, reference = diabetes_test$outcome, positive =  "True")
#Evaluation
confusionMatrix(diabetes_pred_NB, reference = diabetes_test$outcome, positive =  "True")
confusionMatrix(diabetes_pred_NB, reference = diabetes_test$outcome, positive =  "True")
#Evaluation
diabetes_pred_NB <- predict(diabetes_NB_model, newdata = diabetes_test)
confusionMatrix(diabetes_pred_NB, reference = diabetes_test$outcome, positive =  "True")
RNGkind(sample.kind = "Rounding")
set.seed(123)
# setting the working directory to use the csv file
print(getwd())
setwd("C:/Users/ASUS/OneDrive/Desktop/Study-Material/IIIT-Kottayam/SEM-6/Data warehousing and mining ICS 321/Lab/Lab-9")
#Reading
diabetes_df <- read.csv("diabetes.csv", header = TRUE, stringsAsFactors = T)
#Viewing and exploring
head(diabetes_df)
names(diabetes_df) <- c("pregnancies", "glucose", "bloodpressure", "skinthickness", "insulin", "bmi", "diabetespedigreefunction", "age", "outcome")
diabetes_df$outcome <- as.factor(diabetes_df$outcome) %>% na.omit()
#Change Outcome variables as categorical with level = True (1) or False (0)
diabetes_df$outcome <- factor(diabetes_df$outcome, levels = c(0,1), labels = c("False", "True"))
glimpse(diabetes_df)
head(diabetes_df)
#Dive deeper into the structure of the data
str(diabetes_df)
describe(diabetes_df)
colSums(is.na(diabetes_df))
sum(is.na(diabetes_df))
anyNA(diabetes_df)
#Cleaning
#Replace '0' values into NA
diabetes_df[, 2:7][diabetes_df[, 2:7] == 0] <- NA
#Missing value in our dataset
diabetes_missmap <- missmap(diabetes_df)
#predict the missing values
diabetes_MICE <- mice(diabetes_df[, c("glucose","bloodpressure",
"skinthickness","insulin","bmi")],
method='rf')
diabetes_result <- complete(diabetes_MICE)
#add the predicted missing values into our data set
diabetes_df$glucose <- diabetes_result$glucose
diabetes_df$bloodpressure <- diabetes_result$bloodpressure
diabetes_df$skinthickness <- diabetes_result$skinthickness
diabetes_df$insulin  <- diabetes_result$insulin
diabetes_df$bmi <- diabetes_result$bmi
diabetes_missmap <- missmap(diabetes_df)
#Building a model
#split data into training and test data sets
sliced <- createDataPartition(y = diabetes_df$outcome,p = 0.75,list = FALSE)
diabetes_train <- diabetes_df[sliced,]
diabetes_test <- diabetes_df[-sliced,]
#Check dimensions of the split
prop.table(table(diabetes_df$outcome)) * 100
prop.table(table(diabetes_train$outcome)) * 100
prop.table(table(diabetes_test$outcome)) * 100
#create x_diabetes which holds the predictor variables and y which holds the response variables
x_diabetes = diabetes_train[,-9]
y_diabetes = diabetes_train$outcome
#Our Naive Bayes Model
diabetes_NB_model = train(x_diabetes,y_diabetes,'nb',trControl=trainControl(method='cv',number=10))
diabetes_NB_model
#Evaluation
diabetes_pred_NB <- predict(diabetes_NB_model, newdata = diabetes_test)
confusionMatrix(diabetes_pred_NB, reference = diabetes_test$outcome, positive =  "True")

}
print(z)
}
if(flag){
print(paste(inp,"is prime"))
}else{
print(paste(inp,"is not prime"))
}
}
primeOrNot(inp)
A <- matrix(1:6, nrow=2)
A
B<- matrix(1:6,ncol=2)
B
A%*%B
PL <- iris$Petal.Length
barplot(PL)
hist(PL)
SP <- iris$Species
boxplot(PL ~ SP)
summary(aov(PL ~ SP))
PW <- df$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
PW <- iris$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
heatmap(iris)
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
ma <- as.matrix(iris[, 1:4]) # convert to matrix
disMatarix <- dist(ma)
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
age <- c(23,23,27,27,39,41,49,50,52,54,54,56,57,58,58,60,61)
age <- c(23,23,27,27,39,41,49,50,52,54,54,56,57,58,58,60,61)
fatpercent <- c(9.5,26.5,7.8,17.8,31.4,25.9,27.4,27.2,31.2,34.6,42.5,28.8,33.4,30.2,34.1,32.9,41.2,35.7)
data <- list(A = age,B = fatpercent)
df <- as.data.frame(data)
data
age <- c(23,23,27,27,39,41,47,49,50,52,54,54,56,57,58,58,60,61)
fatpercent <- c(9.5,26.5,7.8,17.8,31.4,25.9,27.4,27.2,31.2,34.6,42.5,28.8,33.4,30.2,34.1,32.9,41.2,35.7)
data <- list(A = age,B = fatpercent)
data
df <- as.data.frame(data)
df
data <- list(Age = age,%fat = fatpercent)
data <- list(Age = age,FatPercent = fatpercent)
data
df <- as.data.frame(data)
df
cor(df)
cor(df$Age,df$FatPercent)
# Histogram
hist(iris$Petal.Length,breaks = 30,col="blue", xlab = "Petal Length", main = " Histogram of Pertal Length" )
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
# importing plotting lib
require(ggplot2)
# Density plot
qplot(Petal.Length, data=iris, geom='density')
qplot(Petal.Length, data=iris, geom='density', color=Species, fill=Species)
qplot(Sepal.Length, data=iris, geom='histogram', fill=Species, alpha=I(1/2))
# Density plot
qplot(Petal.Length, data=iris, geom='density')
qplot(Petal.Length, data=iris, geom='density', color=Species, fill=Species)
qplot(Petal.Width, Petal.Length, data=iris, color=Species)
# Scatter plot
qplot(Petal.Width, Petal.Length, data=iris, color=Species)
cor(iris$Sepal.Length, iris$Petal.Length)
cov(iris$Sepal.Length, iris$Petal.Length)
# correlation and covariance
cor(iris$Sepal.Length, iris$Petal.Length)
cov(iris$Sepal.Length, iris$Petal.Length)
cov(iris[,1:4])
df <- iris[, 1:4]
boxplot(df)
pairs(df)
stars(df)
# making every plot i know of ;)
df <- iris[, 1:4]
boxplot(df)
pairs(df)
stars(df)
boxplot(iris)
scatter.smooth(iris)
matrix(scatter.smooth(iris))
table(iris)
pie(table(iris))
matrix(scatter.smooth(iris))
matrix(scatter.smooth(iris))
scatter.smooth(iris)
matrix(scatter.smooth(iris))
table(iris)
table(iris)
pie(table(iris))
barplot(PL)
hist(PL)
SP <- iris$Species
boxplot(PL ~ SP)
summary(aov(PL ~ SP))
PW <- iris$Petal.Width
plot(PL, PW, col = SP)
abline(lm(PW ~ PL))
ma <- as.matrix(iris[, 1:4]) # convert to matrix
disMatarix <- dist(ma)
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
plot(hclust(disMatarix))
heatmap(ma,scale = "column",RowSideColors = rainbow(3)[iris$Species])
vec1<-c(1.1, 2, 3.0, 4.2)
sum(vec1)
prod(vec1)
mean(vec1)
vec2<-c(1.1,NA, 2, 3.0,NA )
sum(vec2)
prod(vec2)
mean(vec2)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
count(str,"%")
count(str,%)
str_count(str)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str)
str_count("%")
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count("%")
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str,pattern = "%")
install.packages("stringr")
library(stringr)
str = c("$I%Love!R programming %","cs^e%portal", "le%.5%rty.in","join2022%")
str_count(str,pattern = "%")
str_count(str,pattern = 'r')
ar <- array(1:5,c(2,3,4))
ar
dim(ar)
for(i in 1:100){
if(i%%4==0 && i%%5==0){
print("BuyNow ")
}
else if(i%%4==0){
print("Now ")
}
else if(i%%5==0){
print("Buy ")
}
else{
print(i)
print(" ")
}
}
for(i in 1:100){
if(i%%4==0 && i%%5==0){
print("BuyNow")
}
else if(i%%4==0){
print("Now")
}
else if(i%%5==0){
print("Buy")
}
else{
print(i)
}
}
print(Sys.Date())
print(Sys.time())
mtcars
data<-mtcars
View(data)
library(dplyr)
select(data)
data<-mtcars
View(data)
dim(data)
summary(data)
randu()
runif(2,1,32)
floor(runif(2,1,32))
floor(runif(2,1,32))
data %>% select(floor(runif(2,1,32))
data
data %>% select(floor(runif(2,1,32))
summary(data)
data %>% select(floor(runif(2,1,32)) )
data %>% select(floor(runif(2,1,1)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(2,1,11)) )
data %>% select(floor(runif(11,1,11)) )
data %>% select(floor(runif(11,1,11)) )
data %>% row.select(floor(runif(11,1,11)) )
data %>% row(select(floor(runif(11,1,11))) )
data %>% (select(row, floor(runif(11,1,11))) )
data %>% filter(floor(runif(11,1,11)))
data %>% filter(floor(runif(2,1,32)))
data %>% filter(floor(runif(1,1,32)))
data %>% filter(2)
data %>% filter(32)
filter(data,)
filter(data,1)
filter(data,false)
slice_sample(data)
slice_sample(data)
slice_sample(data,n)
slice_sample(data,5)
slice_sample(data,n=5)
sample_frac(data,n=5)
sample_frac(data,n=2)
sample_frac(data,n=2)
slice_sample(data,n=5)
sample_frac(data,n=2)
data %>% select(cyl, hp, wt )
filter(mtcars, am == 1)
filter(mtcars,hp>225)
mean(mtcars$mpg)
median(mtcars$mpg)
arrange(mtcars,desc(hp))
days <- c("Monday","Tuesday","Wednesday","Thrusday","Friday","Saturdat","Sunday")
nrbirds <- c(2,5,0,8,1,2,3)
df<-data.frame(days,nrbirds)
df
dayNumber<-c(1,2,3,4,5,6,7)
x<-c(1,2,3,4,5,6,7)
df$dayNumber <- x
df
df = subset(df,select(days))
df = subset(df,select = days)
df
df = subset(df,select = -c(days))
df
days <- c("Monday","Tuesday","Wednesday","Thrusday","Friday","Saturdat","Sunday")
nrbirds <- c(2,5,0,8,1,2,3)
df<-data.frame(days,nrbirds)
x<-c(1,2,3,4,5,6,7)
df$dayNumber <- x
df = subset(df,select = -c(days))
df
df <- select(df, dayNumber, nrbirds)
df
max_row <- which.max(df$nrbirds)
print(max_row)
df
max_row <- which.max(df$nrbirds)
print(max_row)
df[max_row]
df[max_row]
most_birds_daynumber <- df$dayNumber[max_row]
print(most_birds_daynumber)
df <- arrange(df, desc(nrbirds))
df
library(dplyr)
library(stringr)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
# Abhijit Mishra 2020BCS0094
print("Abhijit Mishra")
# Importing the dataset
dataset = read.csv("social.csv")
setwd("C:/Users/ASUS/OneDrive/Desktop/Study-Material/IIIT-Kottayam/SEM-6/Data warehousing and mining ICS 321/Lab/Lab-11")
# Importing the dataset
dataset = read.csv("social.csv")
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
# Abhijit Mishra 2020BCS0094
print("Abhijit Mishra")
# setting the working directory to use the csv file
print(getwd())
setwd("C:/Users/ASUS/OneDrive/Desktop/Study-Material/IIIT-Kottayam/SEM-6/Data warehousing and mining ICS 321/Lab/Lab-11")
# Importing the dataset
dataset = read.csv("social.csv")
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
install.packages('e1071')
library(e1071)
# Fitting SVM to the Training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
library(tidyverse)
# Plotting the training data set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4',Â 'red3'))
# Abhijit Mishra 2020BCS0094
print("Abhijit Mishra")
# setting the working directory to use the csv file
print(getwd())
setwd("C:/Users/ASUS/OneDrive/Desktop/Study-Material/IIIT-Kottayam/SEM-6/Data warehousing and mining ICS 321/Lab/Lab-11")
# Importing the dataset
dataset = read.csv("social.csv")
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
library(tidyverse)
# Plotting the training data set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
library(tidyverse)
# Plotting the training data set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4',Â 'red3'))
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test_set
test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
# Implementation of KNN Algorithm in R
loan <- read.csv("Credit.csv")
str(loan)
loan.subset <- loan[c('Creditability','Age..years.','Sex...Marital.Status','Occupation','Account.Balance','Credit.Amount','Length.of.current.employment','Purpose')]
str(loan.subset)
head(loan.subset)
#Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
loan.subset.n <- as.data.frame(lapply(loan.subset[,2:8], normalize))
head(loan.subset.n)
set.seed(123)
dat.d <- sample(1:nrow(loan.subset.n),size=nrow(loan.subset.n)*0.7,replace = FALSE) #random selection of 70% data.
train.loan <- loan.subset[dat.d,] # 70% training data
test.loan <- loan.subset[-dat.d,] # remaining 30% test data
#Creating seperate dataframe for 'Creditability' feature which is our target.
train.loan_labels <- loan.subset[dat.d,1]
test.loan_labels <-loan.subset[-dat.d,1]
#Install class package
install.packages('class')
install.packages("class")
# Load class package
library(class)
#Find the number of observation
NROW(train.loan_labels)
knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)
knn.27 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=27)
#Calculate the proportion of correct classification for k = 26, 27
ACC.26 <- 100 * sum(test.loan_labels == knn.26)/NROW(test.loan_labels)
ACC.27 <- 100 * sum(test.loan_labels == knn.27)/NROW(test.loan_labels)
library(class)
#Find the number of observation
NROW(train.loan_labels)
knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)
detach("package:e1071", unload = TRUE)
#Find the number of observation
NROW(train.loan_labels)
knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)
# Implementation of KNN Algorithm in R
loan <- read.csv("Credit.csv")
str(loan)
loan.subset <- loan[c('Creditability','Age..years.','Sex...Marital.Status','Occupation','Account.Balance','Credit.Amount','Length.of.current.employment','Purpose')]
str(loan.subset)
head(loan.subset)
#Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
loan.subset.n <- as.data.frame(lapply(loan.subset[,2:8], normalize))
head(loan.subset.n)
set.seed(123)
dat.d <- sample(1:nrow(loan.subset.n),size=nrow(loan.subset.n)*0.7,replace = FALSE) #random selection of 70% data.
train.loan <- loan.subset[dat.d,] # 70% training data
test.loan <- loan.subset[-dat.d,] # remaining 30% test data
#Creating seperate dataframe for 'Creditability' feature which is our target.
train.loan_labels <- loan.subset[dat.d,1]
test.loan_labels <-loan.subset[-dat.d,1]
#Install class package
install.packages('class')
install.packages("class")
# Load class package
library(class)
#Find the number of observation
NROW(train.loan_labels)
knn.26 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=26)
knn.27 <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=27)
#Calculate the proportion of correct classification for k = 26, 27
ACC.26 <- 100 * sum(test.loan_labels == knn.26)/NROW(test.loan_labels)
ACC.27 <- 100 * sum(test.loan_labels == knn.27)/NROW(test.loan_labels)
table(knn.26 ,test.loan_labels)
install.packages('caret')
library(caret)
confusionMatrix(table(knn.26 ,test.loan_labels))
i=1
i=1
k.optm=1
for (i in 1:28){
knn.mod <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=i)
k.optm[i] <- 100 * sum(test.loan_labels == knn.mod)/NROW(test.loan_labels)
k=i
cat(k,'=',k.optm[i],'')}
#Accuracy plot
plot(k.optm, type="b", xlab="K- Value",ylab="AccuracyÂ level")
